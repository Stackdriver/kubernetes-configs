apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: stackdriver-log-aggregator
  namespace: stackdriver-agents
  labels:
    app: stackdriver-log-aggregator
    cluster-level: "true"
spec:
  replicas: 1
  selector:
    matchLabels:
      app: stackdriver-log-aggregator
      cluster-level: "true"
  template:
    metadata:
      labels:
        app: stackdriver-log-aggregator
        cluster-level: "true"
    spec:
      containers:
      - name: stackdriver-log-aggregator
        image: us.gcr.io/stackdriver-kubernetes-1337/stackdriver-logging-agent:lingshi-multi-new
        imagePullPolicy: IfNotPresent
        env:
        - name: GOOGLE_APPLICATION_CREDENTIALS
          valueFrom:
            configMapKeyRef:
              name: google-cloud-config
              key: credentials_path
        - name: CLUSTER_NAME
          valueFrom:
            configMapKeyRef:
              name: cluster-config
              key: cluster_name
        - name: CLUSTER_LOCATION
          valueFrom:
            configMapKeyRef:
              name: cluster-config
              key: cluster_location
        livenessProbe:
          exec:
            command:
            - /bin/sh
            - -c
            - |
              LIVENESS_THRESHOLD_SECONDS=${LIVENESS_THRESHOLD_SECONDS:-300}; STUCK_THRESHOLD_SECONDS=${LIVENESS_THRESHOLD_SECONDS:-900}; if [ ! -e /var/log/k8s-fluentd-buffers ]; then
                exit 1;
              fi; touch -d "${STUCK_THRESHOLD_SECONDS} seconds ago" /tmp/marker-stuck; if [[ -z "$(find /var/log/k8s-fluentd-buffers -type f -newer /tmp/marker-stuck -print -quit)" ]]; then
                rm -rf /var/log/fluentd-buffers;
                exit 1;
              fi; touch -d "${LIVENESS_THRESHOLD_SECONDS} seconds ago" /tmp/marker-liveness; if [[ -z "$(find /var/log/k8s-fluentd-buffers -type f -newer /tmp/marker-liveness -print -quit)" ]]; then
                exit 1;
              fi;
          failureThreshold: 3
          initialDelaySeconds: 600
          periodSeconds: 60
          successThreshold: 1
          timeoutSeconds: 1
        resources:
          limits:
            cpu: 8000m
            memory: 8000Mi
          requests:
            cpu: 2000m
            memory: 1000Mi
        terminationMessagePath: /dev/termination-log
        terminationMessagePolicy: File
        ports:
        - containerPort: 8989
          hostPort: 8989
          protocol: TCP
        volumeMounts:
        - mountPath: /var/log
          name: varlog
        - mountPath: /etc/google-fluentd/google-fluentd.conf
          subPath: google-fluentd.conf
          name: output-config-volume
        - mountPath: /etc/google-fluentd/config.d
          name: input-config-volume
        - mountPath: /etc/google-cloud/
          name: google-cloud-config
      dnsPolicy: ClusterFirst
      restartPolicy: Always
      schedulerName: default-scheduler
      securityContext: {}
      tolerations:
      - operator: "Exists"
        effect: "NoExecute"
      - operator: "Exists"
        effect: "NoSchedule"
      volumes:
      - hostPath:
          path: /var/log
          type: ""
        name: varlog
      - configMap:
          defaultMode: 420
          name: stackdriver-log-aggregator-output-config
        name: output-config-volume
      - configMap:
          defaultMode: 420
          name: stackdriver-log-aggregator-input-config
        name: input-config-volume
      - configMap:
          defaultMode: 420
          name: google-cloud-config
        name: google-cloud-config
  strategy:
    rollingUpdate:
      maxUnavailable: 1
    type: RollingUpdate
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: stackdriver-log-aggregator-input-config
  namespace: stackdriver-agents
data:
  forward.input.conf: |-
    <source>
      @type forward
      port 8989
      bind 0.0.0.0
    </source>

    <match k8s_container.**>
      @type record_modifier
      <record>
        # Extract local_resource_id from record["kubernetes"] for
        # 'k8s_container' monitored resource. The format is:
        # 'k8s_container.<namespace_name>.<pod_name>.<container_name>'.
        "logging.googleapis.com/local_resource_id" ${"k8s_container.#{record["kubernetes"]["namespace_name"]}.#{record["kubernetes"]["pod_name"]}.#{record["kubernetes"]["container_name"]}"}
        # Rename the field 'log' to a more generic field 'message'. This way
        # the fluent-plugin-google-cloud knows to flatten the field as
        # textPayload instead of jsonPayload after extracting 'time',
        # 'severity' and 'stream' from the record.
        # Also trim the entries which exceed slightly less than 100KB, to
        # avoid dropping them. It is a necessity, because Stackdriver only
        # supports entries that are up to 100KB in size.
        message ${record['log'].length > 100000 ? "[Trimmed]#{record['log'][0..100000]}..." : record['log']}
        # If 'severity' is not set, assume stderr is ERROR and stdout is INFO.
        severity ${record['severity'] || if record['stream'] == 'stderr' then 'ERROR' else 'INFO' end}
        # Extract "kubernetes"->"labels" and set them as
        # "logging.googleapis.com/labels". Prefix these labels with "k8s-pod/"
        # to distinguish with other labels and avoid label name collision with
        # other types of labels.
        _dummy_labels_ ${if record.is_a?(Hash) && record.has_key?('kubernetes') && record['kubernetes'].has_key?('labels') && record['kubernetes']['labels'].is_a?(Hash); then; record["logging.googleapis.com/labels"] = record['kubernetes']['labels'].map{ |k, v| ["k8s-pod/#{k}", v]}.to_h; end; nil}
        # This filter parses the 'source' field created for glog lines into a
        # single top-level field, for proper processing by the output plugin.
        # For example, if a record includes:
        #     {"source":"handlers.go:131"},
        # then the following entry will be added to the record:
        #     {"logging.googleapis.com/sourceLocation":
        #          {"file":"handlers.go", "line":"131"}
        #     }
        _dummy_source_location_ ${if record.is_a?(Hash) && record.has_key?('source') && record['source'].include?(':'); then; source_parts = record['source'].split(':', 2); record['logging.googleapis.com/sourceLocation'] = {'file' => source_parts[0], 'line' => source_parts[1]} else; nil; end}
      </record>
      tag ${if record['stream'] == 'stderr' then 'reform.stderr' else 'reform.stdout' end}
      remove_keys kubernetes,log,stream,_dummy_labels_,_dummy_source_location_
    </match>

    ## Detect exceptions in the log output and forward them as one log entry.
    #<match **>
    #  @type detect_exceptions

    #  remove_tag_prefix multi
    #  message message
    #  stream "logging.googleapis.com/local_resource_id"
    #  multiline_flush_interval 5
    #  max_bytes 500000
    #  max_lines 1000
    #</match>

    # Add a unique insertId to each log entry that doesn't already have it.
    # This helps guarantee the order and prevent log duplication.
    <filter **>
      @type add_insert_ids
    </filter>
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: stackdriver-log-aggregator-output-config
  namespace: stackdriver-agents
data:
  google-fluentd.conf: |-
    @include config.d/*.conf

    <system>
      workers 20
      root_dir /var/log/fluentd
    </system>

    # Do not collect fluentd's own logs to avoid infinite loops.
    <match fluent.**>
      @type null
    </match>

    <match **>
      #@type file
      #path /var/log/fluentd/file.%Y%m%d
      @type google_cloud
      @id google_cloud

      # Try to detect JSON formatted log entries.
      detect_json true
      # Allow log entries from multiple containers to be sent in the same
      # request.
      split_logs_by_tag false
      <buffer>
        # Set the buffer type to file to improve the reliability and reduce the
        # memory consumption.
        @type file
        # The max size of each chunks: events will be written into chunks until
        # the size of chunks become this size
        # Set the chunk limit conservatively to avoid exceeding the recommended
        # chunk size of 5MB per write request.
        chunk_limit_size 512k
        # Limit the number of queued chunks.
        queued_chunks_limit_size 20
        # Block processing of input plugin to emit events into that buffer.
        overflow_action block
        # Never wait more than 5 seconds before flushing logs in the non-error
        # case.
        flush_interval 5s
        # Seconds to wait before next retry to flush.
        retry_wait 5s
        # The base number of exponential backoff for retries.
        retry_exponential_backoff_base 5
        # The maximum interval seconds for exponential backoff between retries
        # while failing.
        retry_max_interval 1200
      </buffer>
      # Use multiple threads for processing.
      num_threads 2
      use_grpc true
      k8s_cluster_name "#{ENV["CLUSTER_NAME"]}"
      k8s_cluster_location "#{ENV["CLUSTER_LOCATION"]}"
      adjust_invalid_timestamps false
    </match>
